# ğŸ§ª clinRAG

a langgraph-based RAG chatbot that performs semantic search over chunked clinical trial data (from [clinicaltrials.gov](https://clinicaltrials.gov))  

lightweight streamlit deployment @ [clinrag.streamlit.app](https://clinrag.streamlit.app)

---

## ğŸ”„ flow

- ğŸ“„ parses and chunks (partial, for now) trial data from about ~1000 trials (i am on mongoDB free tier)  
- ğŸ§  embeds clinical trial chunks using `intfloat/e5-large-v2` (you can swap it out in `.env` if you're running locally)  
- ğŸ—„ï¸ stores data in mongoDB with a vector search index  
- ğŸ” performs quick semantic search over the embedded chunks, with filtering based on:

  ```
  nctId
  status
  startDate (before/after)
  completionDate (before/after)
  studyType
  allocation
  interventionModel
  maskingType
  healthyVolunteers
  sex
  stdAges
  ```

- ğŸ›ï¸ pretty streamlit ui for you to try out

---

### ğŸ”¬ overview of clinical trial parts:

1. **ğŸ“˜ OVERVIEW**  
 - general info: titles, description, status, dates, link to full study  
 - from `identificationModule`, `statusModule`, `descriptionModule`

2. **ğŸ§ª DESIGN**  
 - study type, phases, intervention model, allocation, masking, enrollment  
 - from `designModule`, `designInfo`, `maskingInfo`, `enrollmentInfo`

3. **ğŸ§â€â™‚ï¸ ELIGIBILITY**  
 - participant criteria: age range, sex, healthy volunteer status  
 - from `eligibilityModule`

4. **ğŸ§¬ CONDITIONS**  
 - conditions studied + related keywords  
 - from `conditionsModule`

5. **ğŸ§« ARMS & INTERVENTIONS**  
 - experimental/control groups and interventions (drugs, devices, etc)  
 - from `armsInterventionsModule`

6. **ğŸ“Š OUTCOMES**  
 - primary/secondary outcomes: whatâ€™s being measured, when, and how  
 - from `outcomesModule` inside `protocolSection`

---

each of these parts is:
- ğŸ§± assembled into a text block  
- ğŸ§  embedded via `SentenceTransformer`  
- ğŸ“¦ packaged into a `Chunk` with metadata
  
---

## ğŸ§¾ environment variables

create a `.env` file in the root directory like so:

```env
MONGODB_URI=<your cluster uri>
OPENAI_API_KEY=<your api key>
DATABASE_NAME=<self explanatory>
COLLECTION_NAME=<also self explanatory>
EMBEDDING_MODEL=<the sentence-transformers model you're using>
VECTOR_SEARCH_INDEX=<your mongoDB index name>
```

---

## ğŸ› ï¸ setup

### ğŸ“‹ requirements

- ğŸ python 3.12+  
- âš¡ [`uv`](https://github.com/astral-sh/uv) â€” a better pip  
- â˜ï¸ a mongoDB cluster  

---

### ğŸ“¦ installing dependencies

```bash
uv sync
```

---

## ğŸ§¹ preprocessing

1. **fetch + chunk**  
   ğŸ§º pull and preprocess the trial data:

   ```bash
   uv run preprocessing/fetch_and_chunk.py
   ```

2. **initialize your db**  
   ğŸ§Š this inserts the chunks into mongoDB:

   ```bash
   uv run db_init.py
   ```

---

## ğŸ§  setting up vector search

you **must** create a vector search index in mongoDB atlas matching the schema in `extras/vector_index.json`  
âš ï¸ double-check that your `.env` variables (`MONGODB_URI`, `DATABASE_NAME`, etc) are correct

---

## ğŸš€ run the app

```bash
streamlit run app.py
```

---
